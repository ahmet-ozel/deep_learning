{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58686736",
   "metadata": {},
   "source": [
    "![image.png](img/Picture1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8f37a",
   "metadata": {},
   "source": [
    "In our previous discussion, we explained how to obtain the output of a perceptron. We multiply the inputs by the weights, sum them up, add the bias, and then pass the result through the activation function to obtain the output. We emphasized that the weights and bias are crucial in this process. Once we have determined the weights and bias, we can easily obtain the output by providing the desired inputs.\n",
    "\n",
    "Let's illustrate this with an example of housing data. Suppose we have input data representing features of a house (such as square footage, ease of transportation, and age) and the corresponding outputs, which are the house prices. Initially, we randomly assign weights and obtain an output. The difference between this output and the actual house price determines how we update the weights.\n",
    "\n",
    "Now, what happens when the obtained output does not match the actual value? This is where the weights are updated using a technique called backpropagation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ee4e1ec",
   "metadata": {},
   "source": [
    "![image.png](img/Picture2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae489af",
   "metadata": {},
   "source": [
    "To minimize the squared error as much as possible, we need to adjust the values of W (weights) and the bias by small increments or decrements. This process is known as a cost function, where our goal is to bring the squared error as close to zero as possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "984d4400",
   "metadata": {},
   "source": [
    "![image.png](img/Picture3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9856fb5",
   "metadata": {},
   "source": [
    "Let's discuss the variety of activation functions. The relationship between your inputs and outputs can differ significantly. For example, when determining house prices, there is a nearly linearly increasing graph between the inputs (house size) and outputs (price). In this case, the relationship between inputs and outputs is linear. However, when classifying whether an image is a cat or a dog, the relationship between inputs and outputs is not linear. In such cases, where inputs are images and outputs are classes, different types of activation functions come into play. Examples include sigmoid or softmax activation functions.\n",
    "\n",
    "Now, let's address why we square the difference between the predicted value (H(x(i))) and the actual value (Y(i)). If we aimed to equate the difference to zero directly, the resulting function would not be differentiable. By creating a differentiable function, we can find the point where the derivative is minimum, which provides us with the weight and bias values. Therefore, we square the difference to work with a differentiable function. When we plot the cost function graph, we can utilize the derivative of the graph to find the optimal weight and bias values. This process is called backpropagation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e34542e6",
   "metadata": {},
   "source": [
    "![image.png](img/Picture4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9103e",
   "metadata": {},
   "source": [
    "We have discussed activation functions and the backpropagation part. For more detailed information about backpropagation and gradient descent, you can refer to my article on Gradient Descent. It provides a deeper understanding of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abe703",
   "metadata": {},
   "source": [
    "Let's now create an example perceptron to predict house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65d421a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron(num_features, learning_rate=0.01, num_epochs=10):\n",
    "        learning_rate = learning_rate\n",
    "        num_epochs = num_epochs\n",
    "        weights = np.zeros(num_features + 1)  # +1 for bias\n",
    "        \n",
    "def predict(inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        activation = summation\n",
    "        return activation\n",
    "    \n",
    "def train(training_data, labels):\n",
    "        for _ in range(self.num_epochs):\n",
    "            for inputs, label in zip(training_data, labels):\n",
    "                # Forward Propagation\n",
    "                prediction = self.predict(inputs)\n",
    "\n",
    "                # Backpropagation\n",
    "                error = label - prediction #real_y - predicted_y\n",
    "                weights[1:] += self.learning_rate * error * inputs\n",
    "                weights[0] += self.learning_rate * error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983b315",
   "metadata": {},
   "source": [
    "    perceptron function initializes the perceptron with the given number of features, learning rate, and number of epochs. \n",
    "    It creates a variable weights as an array of zeros with a size of num_features + 1 to accommodate the bias term.\n",
    "\n",
    "    predict function takes inputs as input features and calculates the predicted value using the perceptron's weights.\n",
    "    Lineer function formula\n",
    "    \n",
    "    train function trains a perceptron model using the given training data and labels. It iterates through multiple epochs\n",
    "    and updates the weights based on the prediction error using the backpropagation algorithm. The weights are adjusted by adding the product of the learning rate, error, and input features.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fb404c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House Features: [100.   2.  12.] => Predicted Price: 254669.89823791108\n",
      "House Features: [160.   3.  18.] => Predicted Price: 347633.97454922576\n",
      "House Features: [180.   4.  22.] => Predicted Price: 379041.6811784502\n"
     ]
    }
   ],
   "source": [
    "training_data = np.array([[120, 3, 15], [80, 2, 10], [150, 4, 20], [200, 5, 25]])  # House features (square meters, number of rooms, building age)\n",
    "labels = np.array([300000, 200000, 350000, 400000])  # House prices\n",
    "\n",
    "training_data = np.divide(training_data, 100)\n",
    "labels = np.divide(labels, 100) # We need to divide each value\n",
    "\n",
    "perceptron = Perceptron(num_features=3)#To ensure easier calculations and avoid large numbers during multiplication and summation, we divide each number by 100. This allows for more manageable computations.\n",
    "perceptron.train(training_data, labels)\n",
    "\n",
    "# Test\n",
    "test_data = np.array([[100, 2, 12], [160, 3, 18], [180, 4, 22]])\n",
    "test_data = np.divide(test_data, 100)\n",
    "for data in test_data:\n",
    "    prediction = perceptron.predict(data)\n",
    "    print(f\"House Features: {data*100} => Predicted Price: {prediction*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
