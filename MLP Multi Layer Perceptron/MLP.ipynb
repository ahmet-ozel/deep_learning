{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525a5ade",
   "metadata": {},
   "source": [
    "# MLP Multi Layer Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ed78bcf",
   "metadata": {},
   "source": [
    "![1.png](Picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5c60a",
   "metadata": {},
   "source": [
    "In our previous article, we discussed the single-layer perceptron and provided an example code. Now, we will create an MLP (Multi-Layer Perceptron) model using multiple perceptrons. MLP is also known as a DNN (Deep Neural Network) model. DNN encompasses various models such as CNN, LSTM, RNN, and MLP. MLP is just one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b812f",
   "metadata": {},
   "source": [
    "In our previous example, we used a linear activation function. Now, let's use the sigmoid function. So, what are we going to train? Let's say we want to create a model that predicts whether a student will pass a project or not. Our inputs will be the time spent on the project and the grade obtained in the final exam. The output will be 1 if the student passes and 0 if the student fails. Let's prepare a sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "424b62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[10, 80], [15, 100], [50, 59], [9, 49], [25, 80], [30, 85], [26, 77], [5, 75], [42, 91], [3, 70]])\n",
    "y = np.array([[1], [1], [0], [0], [1], [1], [1], [0], [1], [0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54caf8d7",
   "metadata": {},
   "source": [
    "We have created a sample dataset. It is important to remember that the quality of our model will be directly influenced by the quality of the dataset we create. We will discuss how to improve the quality of these datasets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bfca2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, num_input, num_hidden, num_output):\n",
    "        self.num_input = num_input\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_output = num_output\n",
    "        self.hidden_weights = np.random.randn(num_input, num_hidden)\n",
    "        self.hidden_biases = np.zeros((1, num_hidden))\n",
    "        self.output_weights = np.random.randn(num_hidden, num_output)\n",
    "        self.output_biases = np.zeros((1, num_output))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_layer_activation = np.dot(X, self.hidden_weights) + self.hidden_biases\n",
    "        hidden_layer_output = self.sigmoid(hidden_layer_activation)\n",
    "        output_layer_activation = np.dot(hidden_layer_output, self.output_weights) + self.output_biases\n",
    "        predicted_output = self.sigmoid(output_layer_activation)\n",
    "        return predicted_output\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            hidden_layer_activation = np.dot(X, self.hidden_weights) + self.hidden_biases\n",
    "            hidden_layer_output = self.sigmoid(hidden_layer_activation)\n",
    "            output_layer_activation = np.dot(hidden_layer_output, self.output_weights) + self.output_biases\n",
    "            predicted_output = self.sigmoid(output_layer_activation)\n",
    "\n",
    "            # Backpropagation\n",
    "            error = y - predicted_output\n",
    "            d_predicted_output = error * (predicted_output * (1 - predicted_output))\n",
    "            error_hidden_layer = d_predicted_output.dot(self.output_weights.T)\n",
    "            d_hidden_layer = error_hidden_layer * (hidden_layer_output * (1 - hidden_layer_output))\n",
    "\n",
    "            # Updating weights and biases\n",
    "            self.output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "            self.output_biases += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "            self.hidden_weights += X.T.dot(d_hidden_layer) * learning_rate\n",
    "            self.hidden_biases += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ff25176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define MLP\n",
    "mlp = MLP(num_input=2, num_hidden=4, num_output=1)\n",
    "\n",
    "#Train MLP\n",
    "mlp.train(X, y, epochs=10000, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e5b45a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[0.79999705]\n",
      " [0.40001114]]\n"
     ]
    }
   ],
   "source": [
    "test_input = np.array([[50, 100],[20,95]])\n",
    "predicted_output = mlp.forward(test_input)\n",
    "print(\"Predicted:\", predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc934a62",
   "metadata": {},
   "source": [
    "# Now we can try to train our model with softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4af1233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Softmax:\n",
    "    def __init__(self, num_input, num_hidden, num_output):\n",
    "        self.num_input = num_input\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_output = num_output\n",
    "        self.hidden_weights = np.random.randn(num_input, num_hidden)\n",
    "        self.hidden_biases = np.zeros((1, num_hidden))\n",
    "        self.output_weights = np.random.randn(num_hidden, num_output)\n",
    "        self.output_biases = np.zeros((1, num_output))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_layer_activation = np.dot(X, self.hidden_weights) + self.hidden_biases\n",
    "        hidden_layer_output = self.sigmoid(hidden_layer_activation)\n",
    "        output_layer_activation = np.dot(hidden_layer_output, self.output_weights) + self.output_biases\n",
    "        predicted_output = self.sigmoid(output_layer_activation)\n",
    "        return predicted_output\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            hidden_layer_activation = np.dot(X, self.hidden_weights) + self.hidden_biases\n",
    "            hidden_layer_output = self.sigmoid(hidden_layer_activation)\n",
    "            output_layer_activation = np.dot(hidden_layer_output, self.output_weights) + self.output_biases\n",
    "            predicted_output = self.sigmoid(output_layer_activation)\n",
    "\n",
    "            # Backpropagation\n",
    "            error = y - predicted_output\n",
    "            d_predicted_output = error * (predicted_output * (1 - predicted_output))\n",
    "            error_hidden_layer = d_predicted_output.dot(self.output_weights.T)\n",
    "            d_hidden_layer = error_hidden_layer * (hidden_layer_output * (1 - hidden_layer_output))\n",
    "\n",
    "            # Updating weights and biases\n",
    "            self.output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "            self.output_biases += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "            self.hidden_weights += X.T.dot(d_hidden_layer) * learning_rate\n",
    "            self.hidden_biases += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9d30f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define MLP\n",
    "mlp_softmax = MLP_Softmax(num_input=2, num_hidden=4, num_output=1)\n",
    "\n",
    "#Train MLP\n",
    "mlp_softmax.train(X, y, epochs=10000, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89998aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[0.66638842]\n",
      " [0.66639185]]\n"
     ]
    }
   ],
   "source": [
    "test_input = np.array([[50, 100],[20,95]])\n",
    "predicted_output = mlp_softmax.forward(test_input)\n",
    "print(\"Predicted:\", predicted_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
